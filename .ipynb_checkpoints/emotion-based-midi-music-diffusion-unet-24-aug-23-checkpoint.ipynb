{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.status.busy": "2023-08-24T23:24:38.829679Z",
     "iopub.status.idle": "2023-08-24T23:24:38.830587Z",
     "shell.execute_reply": "2023-08-24T23:24:38.830325Z",
     "shell.execute_reply.started": "2023-08-24T23:24:38.830299Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input/'):\n",
    "    for filename in filenames[:]:\n",
    "#         if \".pt\" in filename:\n",
    "            print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T14:19:24.146430Z",
     "iopub.status.busy": "2023-08-22T14:19:24.146031Z",
     "iopub.status.idle": "2023-08-22T14:19:24.151035Z",
     "shell.execute_reply": "2023-08-22T14:19:24.149896Z",
     "shell.execute_reply.started": "2023-08-22T14:19:24.146394Z"
    }
   },
   "outputs": [],
   "source": [
    "# !cp /kaggle/working/ddpm_model.pt /kaggle/working/ddpm_model_with_attention.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T14:19:24.153198Z",
     "iopub.status.busy": "2023-08-22T14:19:24.152789Z",
     "iopub.status.idle": "2023-08-22T14:19:46.634164Z",
     "shell.execute_reply": "2023-08-22T14:19:46.632929Z",
     "shell.execute_reply.started": "2023-08-22T14:19:24.153169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q mido \n",
    "!pip install -q einops imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following MIDI conversion code has been taken from https://github.com/qsdfo/midi_to_numpy/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T14:19:46.639627Z",
     "iopub.status.busy": "2023-08-22T14:19:46.639281Z",
     "iopub.status.idle": "2023-08-22T14:19:46.679320Z",
     "shell.execute_reply": "2023-08-22T14:19:46.678286Z",
     "shell.execute_reply.started": "2023-08-22T14:19:46.639599Z"
    }
   },
   "outputs": [],
   "source": [
    "import mido\n",
    "from mido import MidiFile\n",
    "import numpy as np\n",
    "\n",
    "def write_midi(pr, ticks_per_beat, write_path, tempo=80):\n",
    "    def pr_to_list(pr):\n",
    "        # List event = (pitch, velocity, time)\n",
    "        T, N = pr.shape\n",
    "        t_last = 0\n",
    "        pr_tm1 = np.zeros(N)\n",
    "        list_event = []\n",
    "        for t in range(T):\n",
    "            pr_t = pr[t]\n",
    "            mask = (pr_t != pr_tm1)\n",
    "            if (mask).any():\n",
    "                for n in range(N):\n",
    "                    if mask[n]:\n",
    "                        pitch = n\n",
    "                        velocity = int(pr_t[n])\n",
    "                        # Time is incremented since last event\n",
    "                        t_event = t - t_last\n",
    "                        t_last = t\n",
    "                        list_event.append((pitch, velocity, t_event))\n",
    "            pr_tm1 = pr_t\n",
    "        return list_event\n",
    "    # Tempo\n",
    "    microseconds_per_beat = mido.bpm2tempo(tempo)\n",
    "    # Write a pianoroll in a midi file\n",
    "    mid = MidiFile()\n",
    "    mid.ticks_per_beat = ticks_per_beat\n",
    "\n",
    "    # Each instrument is a track\n",
    "    for instrument_name, matrix in pr.items():\n",
    "        # Add a new track with the instrument name to the midi file\n",
    "        track = mid.add_track(instrument_name)\n",
    "        # transform the matrix in a list of (pitch, velocity, time)\n",
    "        events = pr_to_list(matrix)\n",
    "        # Tempo\n",
    "        track.append(mido.MetaMessage('set_tempo', tempo=microseconds_per_beat))\n",
    "        # Add the program_change\n",
    "        try:\n",
    "            program = program_change_mapping[instrument_name]\n",
    "        except:\n",
    "            # Defaul is piano\n",
    "            # print instrument_name + \" not in the program_change mapping\"\n",
    "            # print \"Default value is 1 (piano)\"\n",
    "            # print \"Check acidano/data_processing/utils/program_change_mapping.py\"\n",
    "            program = 1\n",
    "        track.append(mido.Message('program_change', program=program))\n",
    "\n",
    "        # This list is required to shut down\n",
    "        # notes that are on, intensity modified, then off only 1 time\n",
    "        # Example :\n",
    "        # (60,20,0)\n",
    "        # (60,40,10)\n",
    "        # (60,0,15)\n",
    "        notes_on_list = []\n",
    "        # Write events in the midi file\n",
    "        for event in events:\n",
    "            pitch, velocity, time = event\n",
    "            if velocity == 0:\n",
    "                # Get the channel\n",
    "                track.append(mido.Message('note_off', note=pitch, velocity=0, time=time))\n",
    "                notes_on_list.remove(pitch)\n",
    "            else:\n",
    "                if pitch in notes_on_list:\n",
    "                    track.append(mido.Message('note_off', note=pitch, velocity=0, time=time))\n",
    "                    notes_on_list.remove(pitch)\n",
    "                    time = 0\n",
    "                track.append(mido.Message('note_on', note=pitch, velocity=velocity, time=time))\n",
    "                notes_on_list.append(pitch)\n",
    "    mid.save(write_path)\n",
    "    return\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf8 -*-\n",
    "\n",
    "# from mido import MidiFile\n",
    "from unidecode import unidecode\n",
    "# import numpy as np\n",
    "\n",
    "#######\n",
    "# Pianorolls dims are  :   TIME  *  PITCH\n",
    "\n",
    "\n",
    "class Read_midi(object):\n",
    "    def __init__(self, song_path, quantization):\n",
    "        ## Metadata\n",
    "        self.__song_path = song_path\n",
    "        self.__quantization = quantization\n",
    "\n",
    "        ## Pianoroll\n",
    "        self.__T_pr = None\n",
    "\n",
    "        ## Private misc\n",
    "        self.__num_ticks = None\n",
    "        self.__T_file = None\n",
    "\n",
    "    @property\n",
    "    def quantization(self):\n",
    "        return self.__quantization\n",
    "\n",
    "    @property\n",
    "    def T_pr(self):\n",
    "        return self.__T_pr\n",
    "\n",
    "    @property\n",
    "    def T_file(self):\n",
    "        return self.__T_file\n",
    "\n",
    "    def get_total_num_tick(self):\n",
    "        # Midi length should be written in a meta message at the beginning of the file,\n",
    "        # but in many cases, lazy motherfuckers didn't write it...\n",
    "\n",
    "        # Read a midi file and return a dictionnary {track_name : pianoroll}\n",
    "        mid = MidiFile(self.__song_path)\n",
    "\n",
    "        # Parse track by track\n",
    "        num_ticks = 0\n",
    "        for i, track in enumerate(mid.tracks):\n",
    "            tick_counter = 0\n",
    "            for message in track:\n",
    "                # Note on\n",
    "                time = float(message.time)\n",
    "                tick_counter += time\n",
    "            num_ticks = max(num_ticks, tick_counter)\n",
    "        self.__num_ticks = num_ticks\n",
    "\n",
    "    def get_pitch_range(self):\n",
    "        mid = MidiFile(self.__song_path)\n",
    "        min_pitch = 200\n",
    "        max_pitch = 0\n",
    "        for i, track in enumerate(mid.tracks):\n",
    "            for message in track:\n",
    "                if message.type in ['note_on', 'note_off']:\n",
    "                    pitch = message.note\n",
    "                    if pitch > max_pitch:\n",
    "                        max_pitch = pitch\n",
    "                    if pitch < min_pitch:\n",
    "                        min_pitch = pitch\n",
    "        return min_pitch, max_pitch\n",
    "\n",
    "    def get_time_file(self):\n",
    "        # Get the time dimension for a pianoroll given a certain quantization\n",
    "        mid = MidiFile(self.__song_path)\n",
    "        # Tick per beat\n",
    "        ticks_per_beat = mid.ticks_per_beat\n",
    "        # Total number of ticks\n",
    "        self.get_total_num_tick()\n",
    "        # Dimensions of the pianoroll for each track\n",
    "        self.__T_file = int((self.__num_ticks / ticks_per_beat) * self.__quantization)\n",
    "        return self.__T_file\n",
    "\n",
    "    def read_file(self):\n",
    "        # Read the midi file and return a dictionnary {track_name : pianoroll}\n",
    "        mid = MidiFile(self.__song_path)\n",
    "        # Tick per beat\n",
    "        ticks_per_beat = mid.ticks_per_beat\n",
    "\n",
    "        # Get total time\n",
    "        self.get_time_file()\n",
    "        T_pr = self.__T_file\n",
    "        # Pitch dimension\n",
    "        N_pr = 128\n",
    "        pianoroll = {}\n",
    "\n",
    "        def add_note_to_pr(note_off, notes_on, pr):\n",
    "            pitch_off, _, time_off = note_off\n",
    "            # Note off : search for the note in the list of note on,\n",
    "            # get the start and end time\n",
    "            # write it in th pr\n",
    "            match_list = [(ind, item) for (ind, item) in enumerate(notes_on) if item[0] == pitch_off]\n",
    "            if len(match_list) == 0:\n",
    "                print(\"Try to note off a note that has never been turned on\")\n",
    "                # Do nothing\n",
    "                return\n",
    "\n",
    "            # Add note to the pr\n",
    "            pitch, velocity, time_on = match_list[0][1]\n",
    "            pr[time_on:time_off, pitch] = velocity\n",
    "            # Remove the note from notes_on\n",
    "            ind_match = match_list[0][0]\n",
    "            del notes_on[ind_match]\n",
    "            return\n",
    "\n",
    "        # Parse track by track\n",
    "        counter_unnamed_track = 0\n",
    "        for i, track in enumerate(mid.tracks):\n",
    "            # Instanciate the pianoroll\n",
    "            pr = np.zeros([T_pr, N_pr])\n",
    "            time_counter = 0\n",
    "            notes_on = []\n",
    "            for message in track:\n",
    "\n",
    "                ##########################################\n",
    "                ##########################################\n",
    "                ##########################################\n",
    "                # TODO : keep track of tempo information\n",
    "                # import re\n",
    "                # if re.search(\"tempo\", message.type):\n",
    "                #     import pdb; pdb.set_trace()\n",
    "                ##########################################\n",
    "                ##########################################\n",
    "                ##########################################\n",
    "\n",
    "\n",
    "                # print message\n",
    "                # Time. Must be incremented, whether it is a note on/off or not\n",
    "                time = float(message.time)\n",
    "                time_counter += time / ticks_per_beat * self.__quantization\n",
    "                # Time in pr (mapping)\n",
    "                time_pr = int(round(time_counter))\n",
    "                # Note on\n",
    "                if message.type == 'note_on':\n",
    "                    # Get pitch\n",
    "                    pitch = message.note\n",
    "                    # Get velocity\n",
    "                    velocity = message.velocity\n",
    "                    if velocity > 0:\n",
    "                        notes_on.append((pitch, velocity, time_pr))\n",
    "                    elif velocity == 0:\n",
    "                        add_note_to_pr((pitch, velocity, time_pr), notes_on, pr)\n",
    "                # Note off\n",
    "                elif message.type == 'note_off':\n",
    "                    pitch = message.note\n",
    "                    velocity = message.velocity\n",
    "                    add_note_to_pr((pitch, velocity, time_pr), notes_on, pr)\n",
    "\n",
    "            # We deal with discrete values ranged between 0 and 127\n",
    "            #     -> convert to int\n",
    "            pr = pr.astype(np.int16)\n",
    "            if np.sum(np.sum(pr)) > 0:\n",
    "                name = unidecode(track.name)\n",
    "                name = name.rstrip('\\x00')\n",
    "                if name == u'':\n",
    "                    name = 'unnamed' + str(counter_unnamed_track)\n",
    "                    counter_unnamed_track += 1\n",
    "                if name in pianoroll.keys():\n",
    "                    # Take max of the to pianorolls\n",
    "                    pianoroll[name] = np.maximum(pr, pianoroll[name])\n",
    "                else:\n",
    "                    pianoroll[name] = pr\n",
    "        return pianoroll\n",
    "\n",
    "\n",
    "\n",
    "def get_pianoroll_time(pianoroll):\n",
    "    T_pr_list = []\n",
    "    for k, v in pianoroll.items():\n",
    "        T_pr_list.append(v.shape[0])\n",
    "    if not len(set(T_pr_list)) == 1:\n",
    "        print(\"Inconsistent dimensions in the new PR\")\n",
    "        return None\n",
    "    return T_pr_list[0]\n",
    "\n",
    "def get_pitch_dim(pianoroll):\n",
    "    N_pr_list = []\n",
    "    for k, v in pianoroll.items():\n",
    "        N_pr_list.append(v.shape[1])\n",
    "    if not len(set(N_pr_list)) == 1:\n",
    "        print(\"Inconsistent dimensions in the new PR\")\n",
    "        raise NameError(\"Pr dimension\")\n",
    "    return N_pr_list[0]\n",
    "\n",
    "def dict_to_matrix(pianoroll):\n",
    "    T_pr = get_pianoroll_time(pianoroll)\n",
    "    N_pr = get_pitch_dim(pianoroll)\n",
    "    rp = np.zeros((T_pr, N_pr), dtype=np.int16)\n",
    "    for k, v in pianoroll.items():\n",
    "        if rp.sum() < v.sum():\n",
    "            rp = v\n",
    "    return rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T14:19:46.683583Z",
     "iopub.status.busy": "2023-08-22T14:19:46.683233Z",
     "iopub.status.idle": "2023-08-22T14:19:46.694325Z",
     "shell.execute_reply": "2023-08-22T14:19:46.693305Z",
     "shell.execute_reply.started": "2023-08-22T14:19:46.683559Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import of libraries\n",
    "import random\n",
    "import imageio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import einops\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda\n",
    "from torchvision.datasets.mnist import MNIST, FashionMNIST\n",
    "\n",
    "# import torch_xla.core.xla_model as xm\n",
    "# dev = xm.xla_device()\n",
    "\n",
    "# Setting reproducibility\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Definitions\n",
    "STORE_PATH = \"/kaggle/working/ddpm_model.pt\"\n",
    "store_path = \"/kaggle/working/ddpm_model.pt\"\n",
    "\n",
    "ngpu = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T14:19:46.696577Z",
     "iopub.status.busy": "2023-08-22T14:19:46.696172Z",
     "iopub.status.idle": "2023-08-22T14:19:46.800802Z",
     "shell.execute_reply": "2023-08-22T14:19:46.799538Z",
     "shell.execute_reply.started": "2023-08-22T14:19:46.696544Z"
    }
   },
   "outputs": [],
   "source": [
    "npy_loc = '/kaggle/input/emopia-ported/EMOPIA_1078__song_quads.npy'\n",
    "# npy_loc = '/kaggle/input/midi-numpy-array/midi_single_channel_2501__ALL__song_tags_artists.npy'\n",
    "data = np.load(npy_loc,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lakhmidi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-24T23:24:38.024831Z",
     "iopub.status.busy": "2023-08-24T23:24:38.024299Z",
     "iopub.status.idle": "2023-08-24T23:24:38.042329Z",
     "shell.execute_reply": "2023-08-24T23:24:38.040854Z",
     "shell.execute_reply.started": "2023-08-24T23:24:38.024796Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "def getPreTrainingDataloader():\n",
    "    min_siz = 9999999\n",
    "    audio_data_arr = data.item().get('sig') \n",
    "    audio_q_arr = data.item().get('q')\n",
    "\n",
    "    empty_count = 0\n",
    "\n",
    "    t_size = 192\n",
    "    for i,a in enumerate(audio_data_arr):\n",
    "\n",
    "        a = np.array(a)\n",
    "\n",
    "\n",
    "        for s in [t_size,t_size*2,t_size*3,t_size*4,t_size*5,t_size*6,t_size*7,t_size*8]:\n",
    "\n",
    "            tsr = a[s-t_size:int(s),:]\n",
    "            # Rescaling\n",
    "            tsr = (tsr / (127/2)) - 1\n",
    "\n",
    "            # Data filtering to remove clips that lead/end with silence\n",
    "            if tsr[:10].sum() == 0 or tsr[-10:].sum() == 0:\n",
    "                empty_count+=1\n",
    "                continue\n",
    "\n",
    "            if tsr.shape == (t_size,128):\n",
    "                new_arr.append(torch.Tensor( tsr.reshape(1,-1, 128) ))\n",
    "    \n",
    "    print(\"Total Clips:\", len(new_arr))\n",
    "#     print(len(new_q_arr))\n",
    "    print(\"Empty:\", empty_count)\n",
    "\n",
    "    dataset = TensorDataset(torch.stack(new_arr))#, transform=transform)\n",
    "    loader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "    \n",
    "    del new_arr\n",
    "    del new_q_arr\n",
    "    del audio_data_arr\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMOPIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T14:19:46.824085Z",
     "iopub.status.busy": "2023-08-22T14:19:46.823582Z",
     "iopub.status.idle": "2023-08-22T14:19:46.837008Z",
     "shell.execute_reply": "2023-08-22T14:19:46.835918Z",
     "shell.execute_reply.started": "2023-08-22T14:19:46.824050Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def getEmopiaDataloader(quadrant):\n",
    "    min_siz = 9999999\n",
    "\n",
    "    audio_data_arr = data.item().get('sig') \n",
    "    audio_q_arr = data.item().get('q')\n",
    "    # audio_tag_arr = data.item().get('tags')\n",
    "\n",
    "    new_arr = []\n",
    "    new_q_arr = []\n",
    "    \n",
    "    empty_count = 0\n",
    "    \n",
    "    for i,a in enumerate(audio_data_arr):\n",
    "        q = audio_q_arr[i]\n",
    "        if q != quadrant:\n",
    "            continue\n",
    "        a = np.array(a)\n",
    "\n",
    "        for s in [192,192*2,192*3,192*4,192*5,192*6,192*7,192*8]:\n",
    "\n",
    "            tsr = a[:,s-192:int(s),:]\n",
    "            # Rescaling\n",
    "            tsr = (tsr / (127/2)) - 1\n",
    "\n",
    "            # Data filtering to remove clips that lead/end with silence\n",
    "    #         if tsr[:10].sum() == 0 or tsr[-10:].sum() == 0:\n",
    "    #             empty_count+=1\n",
    "    #             continue\n",
    "\n",
    "            if tsr.shape == (1,192,128):\n",
    "                new_arr.append(torch.Tensor( tsr.reshape(1,-1, 128) ))\n",
    "                new_q_arr.append(q)\n",
    "\n",
    "\n",
    "    # new_arr = new_arr[:96]\n",
    "\n",
    "    print(len(new_arr))\n",
    "    print(len(new_q_arr))\n",
    "    print(empty_count)\n",
    "\n",
    "\n",
    "    dataset = TensorDataset(torch.stack(new_arr))#, transform=transform)\n",
    "    loader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "    del new_arr\n",
    "    del new_q_arr\n",
    "    del audio_data_arr\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-24T23:24:38.045399Z",
     "iopub.status.busy": "2023-08-24T23:24:38.044658Z",
     "iopub.status.idle": "2023-08-24T23:24:38.055131Z",
     "shell.execute_reply": "2023-08-24T23:24:38.054254Z",
     "shell.execute_reply.started": "2023-08-24T23:24:38.045365Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualise_op(images, title=\"\"):\n",
    "    \"\"\"Shows the provided images as sub-pictures in a square\"\"\"\n",
    "    print(images.shape)\n",
    "    # Converting images to CPU numpy arrays\n",
    "    if type(images) is torch.Tensor:\n",
    "        images = images.detach().cpu().numpy()\n",
    "\n",
    "    # Defining number of rows and columns\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    rows = int(len(images) ** (1 / 2))\n",
    "    cols = round(len(images) / rows)\n",
    "\n",
    "    # Populating figure with sub-plots\n",
    "    idx = 0\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            fig.add_subplot(rows, cols, idx + 1)\n",
    "\n",
    "            if idx < len(images):\n",
    "                plt.imshow(images[idx][0], norm=None)\n",
    "                idx += 1\n",
    "    fig.suptitle(title, fontsize=30)\n",
    "\n",
    "    # Showing the figure\n",
    "    plt.show()\n",
    "\n",
    "# Shows the first batch of images\n",
    "def show_first_batch(loader):\n",
    "    for batch in loader:\n",
    "        visualise_op(batch[0], \"Images in the first batch\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T14:19:46.861450Z",
     "iopub.status.busy": "2023-08-22T14:19:46.861075Z",
     "iopub.status.idle": "2023-08-22T14:19:46.873039Z",
     "shell.execute_reply": "2023-08-22T14:19:46.871867Z",
     "shell.execute_reply.started": "2023-08-22T14:19:46.861417Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_new_samples(ddpm, n_samples=4, device=None, c=1, h=128, w=128):\n",
    "    \"\"\"Given a DDPM model, a number of samples to be generated and a device, \n",
    "        returns some newly generated samples\"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if device is None:\n",
    "            device = ddpm.device\n",
    "\n",
    "        # Starting from random noise\n",
    "        x = torch.randn(n_samples, c, h, w).to(device)\n",
    "\n",
    "        for idx, t in enumerate(list(range(ddpm.n_steps))[::-1]):\n",
    "            # Estimating noise to be removed\n",
    "            time_tensor = (torch.ones(n_samples, 1) * t).to(device).long()\n",
    "            eta_theta = ddpm.backward(x, time_tensor)\n",
    "\n",
    "            alpha_t = ddpm.alphas[t]\n",
    "            alpha_t_bar = ddpm.alpha_bars[t]\n",
    "\n",
    "            # Partially denoising the image\n",
    "            x = (1 / alpha_t.sqrt()) * (x - (1 - alpha_t) / (1 - alpha_t_bar).sqrt() * eta_theta)\n",
    "\n",
    "            if t > 0:\n",
    "                z = torch.randn(n_samples, c, h, w).to(device)\n",
    "\n",
    "                beta_t = ddpm.betas[t]\n",
    "                sigma_t = beta_t.sqrt()\n",
    "\n",
    "                x = x + sigma_t * z\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDPMModel\n",
    "This is responsible for storing alpha and beta values & applies the forward process\n",
    "The fwd process just keeps adding noise.\n",
    "the backward process is used for denoising where this class just uses the UNet passed to it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNet\n",
    "Our unet keeps the spatial dimentionality unchanged\n",
    "#### Positional Embeddings\n",
    "The image to image model has to be conditioned on the current timestep\n",
    "We implement this by using sinusoidal embeddings and an MLP layer\n",
    "We use the `_make_te` MLP to map positional embeddings\n",
    "#### Design\n",
    "5 downsample, bottleneck, 5 upsample and residual connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-24T23:24:38.086340Z",
     "iopub.status.busy": "2023-08-24T23:24:38.086050Z",
     "iopub.status.idle": "2023-08-24T23:24:38.812984Z",
     "shell.execute_reply": "2023-08-24T23:24:38.809903Z",
     "shell.execute_reply.started": "2023-08-24T23:24:38.086316Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m    betas \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m (alphas_cumprod[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m/\u001b[39m alphas_cumprod[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     21\u001b[0m    \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mclip(betas, \u001b[38;5;241m0.0001\u001b[39m, \u001b[38;5;241m0.9999\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSelfAttention\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m     24\u001b[0m    \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, channels):\n\u001b[1;32m     25\u001b[0m        \u001b[38;5;28msuper\u001b[39m(SelfAttention, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    " def sinusoidal_embedding(n, d):\n",
    "    # Returns the standard positional embedding\n",
    "    embedding = torch.zeros(n, d)\n",
    "    wk = torch.tensor([1 / 10_000 ** (2 * j / d) for j in range(d)])\n",
    "    wk = wk.reshape((1, d))\n",
    "    t = torch.arange(n).reshape((n, 1))\n",
    "    embedding[:,::2] = torch.sin(t * wk[:,::2])\n",
    "    embedding[:,1::2] = torch.cos(t * wk[:,::2])\n",
    "\n",
    "    return embedding\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    \"\"\"\n",
    "    cosine schedule as proposed in https://arxiv.org/abs/2102.09672\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0.0001, 0.9999)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.channels = channels        \n",
    "        num_heads = 2 if channels == 10 else 4 \n",
    "        self.mha = nn.MultiheadAttention(channels, num_heads, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([channels])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([channels]),\n",
    "            nn.Linear(channels, channels),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(channels, channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.shape[-1]\n",
    "        x = x.view(-1, self.channels, size * size).swapaxes(1, 2)\n",
    "        x_ln = self.ln(x)\n",
    "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
    "        attention_value = attention_value + x\n",
    "        attention_value = self.ff_self(attention_value) + attention_value\n",
    "        return attention_value.swapaxes(2, 1).view(-1, self.channels, size, size)\n",
    "\n",
    "class Empty(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Empty, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "# DDPM class\n",
    "class DDPMModel(nn.Module):\n",
    "    def __init__(self, network, n_steps=200, min_beta=10 ** -4, max_beta=0.02, device=None, image_chw=(1, 192, 128)):\n",
    "        super(DDPMModel, self).__init__()\n",
    "        self.n_steps = n_steps\n",
    "        self.device = device\n",
    "        self.image_chw = image_chw\n",
    "        self.network = network.to(device)\n",
    "        self.betas = torch.linspace(min_beta, max_beta, n_steps).to(device)  # Number of steps is typically in the order of thousands\n",
    "#         self.betas = cosine_beta_schedule(n_steps).to(device)\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alpha_bars = torch.tensor([torch.prod(self.alphas[:i + 1]) for i in range(len(self.alphas))]).to(device)\n",
    "\n",
    "    def forward(self, x0, t, eta=None):\n",
    "        # Make input image more noisy (we can directly skip to the desired step)\n",
    "        n, c, h, w = x0.shape\n",
    "        a_bar = self.alpha_bars[t]\n",
    "\n",
    "        if eta is None:\n",
    "            eta = torch.randn(n, c, h, w).to(self.device)\n",
    "\n",
    "        noisy = a_bar.sqrt().reshape(n, 1, 1, 1) * x0 + (1 - a_bar).sqrt().reshape(n, 1, 1, 1) * eta\n",
    "        return noisy\n",
    "\n",
    "    def backward(self, x, t):\n",
    "        # Run each image through the network for each timestep t in the vector t.\n",
    "        # The network returns its estimation of the noise that was added.\n",
    "        return self.network(x, t)\n",
    "    \n",
    "class DConvBlock(nn.Module):\n",
    "    def __init__(self, shape, in_c, out_c, kernel_size=3, stride=1, padding=1, activation=None, normalize=True):\n",
    "        super(DConvBlock, self).__init__()\n",
    "        self.s_shape = shape\n",
    "        self.in_c = in_c\n",
    "        self.out_c = out_c\n",
    "        self.ln = nn.LayerNorm(shape)\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size, stride, padding)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size, stride, padding)\n",
    "        self.activation = nn.SiLU() if activation is None else activation\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.ln(x) if self.normalize else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "    \n",
    "class MyUNet(nn.Module):\n",
    "    def __init__(self, n_steps, time_emb_dim=100):\n",
    "        super(MyUNet, self).__init__()\n",
    "\n",
    "        ################################################################# Sinusoidal embedding\n",
    "        self.time_embed = nn.Embedding(n_steps, time_emb_dim)\n",
    "        self.time_embed.weight.data = sinusoidal_embedding(n_steps, time_emb_dim)\n",
    "        self.time_embed.requires_grad_(False)\n",
    "\n",
    "        ################################################################ First half\n",
    "        self.te1 = self._make_te(time_emb_dim, 1)\n",
    "        self.b1 = nn.Sequential(\n",
    "            DConvBlock((1, 192, 128), 1, 10, kernel_size=5, padding=2),\n",
    "            DConvBlock((10, 192, 128), 10, 20, kernel_size=5, padding=2),\n",
    "            DConvBlock((20, 192, 128), 20, 20, kernel_size=5, padding=2)\n",
    "        )\n",
    "        self.down1 = nn.Conv2d(20, 20, (6,4), (3,2), (2,1))\n",
    "        \n",
    "        self.te11 = self._make_te(time_emb_dim, 20)\n",
    "        self.b11 = nn.Sequential(\n",
    "            DConvBlock((20, 64, 64), 20, 20),\n",
    "            DConvBlock((20, 64, 64), 20, 20),\n",
    "            DConvBlock((20, 64, 64), 20, 20)\n",
    "        )\n",
    "        self.down11 = nn.Conv2d(20, 20, 4, 2, 1)\n",
    "        \n",
    "        self.te12 = self._make_te(time_emb_dim, 20)\n",
    "        self.b12 = nn.Sequential(\n",
    "            DConvBlock((20, 32, 32), 20, 20),\n",
    "            SelfAttention(20),\n",
    "            DConvBlock((20, 32, 32), 20, 20),\n",
    "            DConvBlock((20, 32, 32), 20, 20)\n",
    "        )\n",
    "        self.down12 = nn.Conv2d(20, 20, 4, 2, 1)\n",
    "        \n",
    "        self.te2 = self._make_te(time_emb_dim, 20)\n",
    "        self.b2 = nn.Sequential(\n",
    "            DConvBlock((20, 16, 16), 20, 20),\n",
    "            SelfAttention(20),\n",
    "            DConvBlock((20, 16, 16), 20, 20),\n",
    "            DConvBlock((20, 16, 16), 20, 20)\n",
    "        )\n",
    "        self.down2 = nn.Conv2d(20, 20, 4, 2, 1)\n",
    "        \n",
    "        self.te3 = self._make_te(time_emb_dim, 20)\n",
    "        self.b3 = nn.Sequential(\n",
    "            DConvBlock((20, 8, 8), 20, 40),\n",
    "            SelfAttention(40),\n",
    "            DConvBlock((40, 8, 8), 40, 40),\n",
    "            DConvBlock((40, 8, 8), 40, 40)\n",
    "        )\n",
    "        self.down3 = nn.Sequential(\n",
    "            nn.Conv2d(40, 40, 2, 1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(40, 40, 4, 2, 1)\n",
    "        )\n",
    "\n",
    "        ######################################################## Bottleneck\n",
    "        self.te_mid = self._make_te(time_emb_dim, 40)\n",
    "        self.b_mid = nn.Sequential(\n",
    "            DConvBlock((40, 3, 3), 40, 20),\n",
    "            SelfAttention(20),\n",
    "            DConvBlock((20, 3, 3), 20, 20),\n",
    "            DConvBlock((20, 3, 3), 20, 40)\n",
    "        )\n",
    "\n",
    "        ######################################################## Second half\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(40, 40, 3, 3, 1),\n",
    "            nn.SiLU(),\n",
    "            nn.ConvTranspose2d(40, 40, 2, 1)\n",
    "        )\n",
    "\n",
    "        self.te4 = self._make_te(time_emb_dim, 80)\n",
    "        self.b4 = nn.Sequential(\n",
    "            DConvBlock((80, 8, 8), 80, 40),\n",
    "            SelfAttention(40),\n",
    "            DConvBlock((40, 8, 8), 40, 20),\n",
    "            DConvBlock((20, 8, 8), 20, 20)\n",
    "        )\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(20, 20, 4, 2, 1)\n",
    "        \n",
    "        self.te5 = self._make_te(time_emb_dim, 40)\n",
    "        self.b5 = nn.Sequential(\n",
    "            DConvBlock((40, 16, 16), 40, 20),\n",
    "            SelfAttention(20),\n",
    "            DConvBlock((20, 16, 16), 20, 20),\n",
    "            DConvBlock((20, 16, 16), 20, 20)\n",
    "        )\n",
    "\n",
    "        self.up21 = nn.ConvTranspose2d(20, 20, 4, 2, 1)\n",
    "        self.te51 = self._make_te(time_emb_dim, 40)\n",
    "        self.b51 = nn.Sequential(\n",
    "            DConvBlock((40, 32, 32), 40, 40),\n",
    "            SelfAttention(40),\n",
    "            DConvBlock((40, 32, 32), 40, 20),\n",
    "            DConvBlock((20, 32, 32), 20, 20)\n",
    "        )\n",
    "\n",
    "        self.up22 = nn.ConvTranspose2d(20, 20, 4, 2, 1)\n",
    "        self.te52 = self._make_te(time_emb_dim, 40)\n",
    "        self.b52 = nn.Sequential(\n",
    "            DConvBlock((40, 64, 64), 40, 40),\n",
    "            DConvBlock((40, 64, 64), 40, 20),\n",
    "            DConvBlock((20, 64, 64), 20, 20)\n",
    "        )\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(20, 20, (7,4), (3,2), (2,1))\n",
    "        self.te_out = self._make_te(time_emb_dim, 40)\n",
    "        self.b_out = nn.Sequential(\n",
    "            DConvBlock((40, 192, 128), 40, 20),\n",
    "            DConvBlock((20, 192, 128), 20, 10),\n",
    "            DConvBlock((10, 192, 128), 10, 10, normalize=False)\n",
    "        )\n",
    "        \n",
    "        self.conv_out = nn.Conv2d(10, 1, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # x is (N, 2, 28, 28) (image with positional embedding stacked on channel dimension)\n",
    "            \n",
    "        \n",
    "        t = self.time_embed(t)\n",
    "        n = len(x)\n",
    "        out1 = self.b1(x + self.te1(t).reshape(n, -1, 1, 1))                          # (N, 10, 128, 128)\n",
    "#         print(out1.shape)\n",
    "#         out1 = self.sa1(out1)\n",
    "        out11 = self.b11(self.down1(out1) + self.te11(t).reshape(n, -1, 1, 1))        # (N, 10, 64, 64)\n",
    "#         out11 = self.sa11(out11)\n",
    "        out12 = self.b12(self.down11(out11) + self.te12(t).reshape(n, -1, 1, 1))      # (N, 10, 32, 32)\n",
    "#         out12 = self.sa12(out12)\n",
    "        out2 = self.b2(self.down12(out12) + self.te2(t).reshape(n, -1, 1, 1))         # (N, 20, 16, 16)\n",
    "#         out2 = self.sa2(out2)\n",
    "        out3 = self.b3(self.down2(out2) + self.te3(t).reshape(n, -1, 1, 1))           # (N, 40, 8, 8)\n",
    "#         out3 = self.sa3(out3)\n",
    "\n",
    "        out_mid = self.b_mid(self.down3(out3) + self.te_mid(t).reshape(n, -1, 1, 1))  # (N, 40, 3, 3)\n",
    "\n",
    "        out4 = torch.cat((out3, self.up1(out_mid)), dim=1)                            # (N, 80, 8, 8)\n",
    "        out4 = self.b4(out4 + self.te4(t).reshape(n, -1, 1, 1))                       # (N, 20, 8, 8)\n",
    "#         out4 = self.sa4(out4)\n",
    "\n",
    "        out5 = torch.cat((out2, self.up2(out4)), dim=1)                               # (N, 40, 16, 16)\n",
    "        out5 = self.b5(out5 + self.te5(t).reshape(n, -1, 1, 1))                       # (N, 10, 16, 16)\n",
    "#         out5 = self.sa5(out5)\n",
    "\n",
    "        out51 = torch.cat((out12, self.up21(out5)), dim=1)                            # (N, 20, 32, 32)\n",
    "        out51 = self.b51(out51 + self.te51(t).reshape(n, -1, 1, 1))                   # (N, 10, 32, 32)\n",
    "#         out51 = self.sa51(out51)\n",
    "\n",
    "        out52 = torch.cat((out11, self.up22(out51)), dim=1)                           # (N, 40, 64, 64)\n",
    "        out52 = self.b52(out52 + self.te52(t).reshape(n, -1, 1, 1))                  # (N, 10, 64, 64)\n",
    "#         out52 = self.sa52(out52)\n",
    "#         print(self.up3(out52).shape)\n",
    "        out = torch.cat((out1, self.up3(out52)), dim=1)                               # (N, 20, 128, 128)\n",
    "        out = self.b_out(out + self.te_out(t).reshape(n, -1, 1, 1))                   # (N, 1, 128, 128)\n",
    "#         out = self.saout(out)\n",
    "\n",
    "        out = self.conv_out(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _make_te(self, dim_in, dim_out):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(dim_in, dim_out),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim_out, dim_out)\n",
    "        )\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "n_steps = 1000 \n",
    "\n",
    "ddpm = DDPMModel(\n",
    "            MyUNet(n_steps), \n",
    "            n_steps=n_steps, \n",
    "            device=device\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-24T23:24:38.816343Z",
     "iopub.status.idle": "2023-08-24T23:24:38.818999Z",
     "shell.execute_reply": "2023-08-24T23:24:38.818741Z",
     "shell.execute_reply.started": "2023-08-24T23:24:38.818713Z"
    }
   },
   "outputs": [],
   "source": [
    "channels = 1\n",
    "height=192\n",
    "\n",
    "def training_loop(ddpm, loader, n_epochs, optim, device, display=False, store_path=store_path, best_loss=float(\"inf\"), n_steps=1000):\n",
    "    mse = nn.MSELoss()\n",
    "#     n_steps = ddpm.n_steps\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs), desc=f\"Training progress\", colour=\"#00ff00\"):\n",
    "        epoch_loss = 0.0\n",
    "        for step, batch in enumerate(tqdm(loader, leave=False, desc=f\"Epoch {epoch + 1}/{n_epochs}\", colour=\"#005500\")):\n",
    "#             print('start stepp', step)\n",
    "            # Loading data\n",
    "            x0 = batch[0].to(device)\n",
    "            n = len(x0)\n",
    "            \n",
    "            # Picking some noise for each of the images in the batch, a timestep and the respective alpha_bars\n",
    "            eta = torch.randn_like(x0).to(device)\n",
    "            t = torch.randint(0, n_steps, (n,)).to(device)\n",
    "            \n",
    "#             print(\"TL\", x0.shape)\n",
    "\n",
    "            # Computing the noisy image based on x0 and the time-step (forward process)\n",
    "            noisy_imgs = ddpm(x0, t, eta)\n",
    "\n",
    "            # Getting model estimation of noise based on the images and the time-step\n",
    "            eta_theta = ddpm.backward(noisy_imgs, t.reshape(n, -1))\n",
    "\n",
    "            # Optimizing the MSE between the noise plugged and the predicted noise\n",
    "            loss = mse(eta_theta, eta)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            epoch_loss += loss.item() * len(x0) / len(loader.dataset)\n",
    "#             print('DONE Step', step)\n",
    "\n",
    "        # Display images generated at this epoch\n",
    "        if display and (epoch+1)%10 == 0:\n",
    "            visualise_op(generate_new_samples(ddpm, device=device, c=channels, h=height), f\"Images generated at epoch {epoch + 1}\")\n",
    "\n",
    "        log = f\"Loss at epoch {epoch + 1} = {epoch_loss:.6f}\"\n",
    "\n",
    "        # Storing the model\n",
    "        if best_loss > epoch_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(ddpm.state_dict(), store_path)\n",
    "#             torch.save(ddpm.state_dict(), '/kaggle/output/pokemon-model.pt')\n",
    "            log += \" --> Model stored\"\n",
    "\n",
    "        print(log)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T14:19:51.885472Z",
     "iopub.status.busy": "2023-08-22T14:19:51.884739Z",
     "iopub.status.idle": "2023-08-22T14:19:51.939880Z",
     "shell.execute_reply": "2023-08-22T14:19:51.936827Z",
     "shell.execute_reply.started": "2023-08-22T14:19:51.885434Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dsdsd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0005\u001b[39m\n\u001b[1;32m      2\u001b[0m best_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdsdsd\u001b[49m\n\u001b[1;32m      5\u001b[0m store_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-----/kaggle/working/ddpm_model_base_without_attention_17Aug.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dsdsd' is not defined"
     ]
    }
   ],
   "source": [
    "lr = 0.0005\n",
    "best_loss = float(\"inf\")\n",
    "# dsdsd\n",
    "\n",
    "store_path = \"/kaggle/working/ddpm_model_base_without_attention_17Aug.pt\"\n",
    "\n",
    "try:\n",
    "    ddpm.load_state_dict(torch.load(store_path, map_location=device))\n",
    "    print('New Model is loaded')\n",
    "    \n",
    "except:\n",
    "#     model_path = '/kaggle/input/midi-numpy-array/ddpm_model.pt'\n",
    "#     ddpm.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    print('No Model')\n",
    "\n",
    "quadrant = 4\n",
    "\n",
    "loader = getDataloader(quadrant)\n",
    "\n",
    "store_path = \"/kaggle/working/ddpm_model_q\" + str(quadrant) + \"_without_attention_17Aug.pt\"\n",
    "    \n",
    "training_loop(ddpm, \n",
    "      loader, \n",
    "      100, \n",
    "      Adam(ddpm.parameters(), lr), \n",
    "      device, \n",
    "      display=False, \n",
    "      store_path=store_path, \n",
    "      best_loss=best_loss,\n",
    "      n_steps=n_steps\n",
    ")\n",
    "\n",
    "# ddpm.load_state_dict(torch.load(\"/kaggle/working/ddpm_model_10Jul.pt\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-22T14:19:51.940815Z",
     "iopub.status.idle": "2023-08-22T14:19:51.941264Z",
     "shell.execute_reply": "2023-08-22T14:19:51.941045Z",
     "shell.execute_reply.started": "2023-08-22T14:19:51.941025Z"
    }
   },
   "outputs": [],
   "source": [
    "# !rm -rf *wqw*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-22T14:19:51.943617Z",
     "iopub.status.idle": "2023-08-22T14:19:51.944635Z",
     "shell.execute_reply": "2023-08-22T14:19:51.944334Z",
     "shell.execute_reply.started": "2023-08-22T14:19:51.944310Z"
    }
   },
   "outputs": [],
   "source": [
    "n_steps=1000\n",
    "best_model = DDPMModel(MyUNet(n_steps), n_steps=n_steps, device=device)\n",
    "best_model.load_state_dict(torch.load(store_path, map_location=device))\n",
    "best_model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save generated midi for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-22T14:19:51.951999Z",
     "iopub.status.idle": "2023-08-22T14:19:51.952403Z",
     "shell.execute_reply": "2023-08-22T14:19:51.952242Z",
     "shell.execute_reply.started": "2023-08-22T14:19:51.952225Z"
    }
   },
   "outputs": [],
   "source": [
    "def hp_filter(a, mean):\n",
    "    if a < mean*7:\n",
    "        return 0\n",
    "    return a\n",
    "\n",
    "def create_midi(audio, channels=1):\n",
    "    audio = audio * (127/audio.max())\n",
    "    audio = audio - audio.min()\n",
    "    audio = audio * (127/audio.max())\n",
    "\n",
    "    audio = audio.reshape(-1)\n",
    "\n",
    "    audio = np.array([hp_filter(a, audio.mean()) for a in audio])\n",
    "    \n",
    "    audio = audio * (127/audio.max())\n",
    "\n",
    "    audio = audio.reshape(1,channels,-1,128)\n",
    "    return audio #audio.astype(int)\n",
    "\n",
    "def midi_post_process(audio, r=192, c=128):\n",
    "    print(audio.shape)\n",
    "    audio = audio.reshape(r,c).T\n",
    "    for i in range(c):\n",
    "        for j in range(r):\n",
    "            if abs(audio[i][j] - audio[i][j-1]) < 127*0.1 and audio[i][j] > 0:\n",
    "                audio[i][j] = audio[i][j-1]\n",
    "        \n",
    "    audio = audio.T\n",
    "    audio = audio.reshape(1,1,r,c)\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-22T14:19:51.954248Z",
     "iopub.status.idle": "2023-08-22T14:19:51.954988Z",
     "shell.execute_reply": "2023-08-22T14:19:51.954754Z",
     "shell.execute_reply.started": "2023-08-22T14:19:51.954730Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/working/'):\n",
    "    for filename in filenames[:]:\n",
    "        weight_path = os.path.join(dirname, filename)\n",
    "        \n",
    "        if \"_q\" in weight_path and \"with_\" in weight_path:\n",
    "            folder_name = weight_path.replace('17Aug.pt', 'files')\n",
    "            !mkdir {folder_name}\n",
    "            n_steps=1000\n",
    "            best_model = DDPMModel(MyUNet(n_steps), n_steps=n_steps, device=device)\n",
    "            best_model.load_state_dict(torch.load(weight_path, map_location=device))\n",
    "            best_model.eval()\n",
    "            print()\n",
    "#             for i in range (20):\n",
    "            save_samples(best_model, folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-22T14:19:51.957104Z",
     "iopub.status.idle": "2023-08-22T14:19:51.957578Z",
     "shell.execute_reply": "2023-08-22T14:19:51.957349Z",
     "shell.execute_reply.started": "2023-08-22T14:19:51.957327Z"
    }
   },
   "outputs": [],
   "source": [
    "# generated = generate_new_samples(\n",
    "#         best_model,\n",
    "#         n_samples=64,\n",
    "#         device=device,\n",
    "#         h=height\n",
    "#     )\n",
    "\n",
    "# # print(generated.max())\n",
    "# # print(generated.min())\n",
    "# # print(generated.mean())\n",
    "\n",
    "# visualise_op(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-22T14:19:51.959439Z",
     "iopub.status.idle": "2023-08-22T14:19:51.959888Z",
     "shell.execute_reply": "2023-08-22T14:19:51.959679Z",
     "shell.execute_reply.started": "2023-08-22T14:19:51.959659Z"
    }
   },
   "outputs": [],
   "source": [
    "# !rm -rf /kaggle/working/*_files\n",
    "# !rm saved_midis*!rm saved_midis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T14:24:40.386606Z",
     "iopub.status.busy": "2023-08-22T14:24:40.385873Z",
     "iopub.status.idle": "2023-08-22T14:24:41.367477Z",
     "shell.execute_reply": "2023-08-22T14:24:41.366313Z",
     "shell.execute_reply.started": "2023-08-22T14:24:40.386570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: RF3/ (stored 0%)\n",
      "  adding: RF3/sample_15.mid (deflated 44%)\n",
      "  adding: RF3/sample_5.mid (deflated 37%)\n",
      "  adding: RF3/sample_8.mid (deflated 43%)\n",
      "  adding: RF3/sample_14.mid (deflated 36%)\n",
      "  adding: RF3/sample_11.mid (deflated 45%)\n",
      "  adding: RF3/sample_10.mid (deflated 37%)\n",
      "  adding: RF3/sample_3.mid (deflated 53%)\n",
      "  adding: RF3/sample_0.mid (deflated 33%)\n",
      "  adding: RF3/sample_6.mid (deflated 47%)\n",
      "  adding: RF3/sample_12.mid (deflated 43%)\n",
      "  adding: RF3/sample_1.mid (deflated 36%)\n",
      "  adding: RF3/sample_2.mid (deflated 31%)\n",
      "  adding: RF3/sample_13.mid (deflated 55%)\n",
      "  adding: RF3/sample_4.mid (deflated 33%)\n",
      "  adding: RF3/sample_9.mid (deflated 33%)\n",
      "  adding: RF3/sample_7.mid (deflated 32%)\n",
      "  adding: RF2/ (stored 0%)\n",
      "  adding: RF2/sample_15.mid (deflated 37%)\n",
      "  adding: RF2/sample_5.mid (deflated 26%)\n",
      "  adding: RF2/sample_8.mid (deflated 55%)\n",
      "  adding: RF2/sample_14.mid (deflated 57%)\n",
      "  adding: RF2/sample_11.mid (deflated 52%)\n",
      "  adding: RF2/sample_10.mid (deflated 55%)\n",
      "  adding: RF2/sample_3.mid (deflated 48%)\n",
      "  adding: RF2/sample_0.mid (deflated 45%)\n",
      "  adding: RF2/sample_6.mid (deflated 54%)\n",
      "  adding: RF2/sample_12.mid (deflated 54%)\n",
      "  adding: RF2/sample_1.mid (deflated 38%)\n",
      "  adding: RF2/sample_2.mid (deflated 50%)\n",
      "  adding: RF2/sample_13.mid (deflated 62%)\n",
      "  adding: RF2/sample_4.mid (deflated 53%)\n",
      "  adding: RF2/sample_9.mid (deflated 61%)\n",
      "  adding: RF2/sample_7.mid (deflated 53%)\n",
      "  adding: RF4/ (stored 0%)\n",
      "  adding: RF4/sample_15.mid (deflated 29%)\n",
      "  adding: RF4/sample_5.mid (deflated 36%)\n",
      "  adding: RF4/sample_8.mid (deflated 52%)\n",
      "  adding: RF4/sample_14.mid (deflated 31%)\n",
      "  adding: RF4/sample_11.mid (deflated 36%)\n",
      "  adding: RF4/sample_10.mid (deflated 42%)\n",
      "  adding: RF4/sample_3.mid (deflated 34%)\n",
      "  adding: RF4/sample_0.mid (deflated 35%)\n",
      "  adding: RF4/sample_6.mid (deflated 38%)\n",
      "  adding: RF4/sample_12.mid (deflated 39%)\n",
      "  adding: RF4/sample_1.mid (deflated 37%)\n",
      "  adding: RF4/sample_2.mid (deflated 39%)\n",
      "  adding: RF4/sample_13.mid (deflated 48%)\n",
      "  adding: RF4/sample_4.mid (deflated 37%)\n",
      "  adding: RF4/sample_9.mid (deflated 36%)\n",
      "  adding: RF4/sample_7.mid (deflated 39%)\n",
      "  adding: RF1/ (stored 0%)\n",
      "  adding: RF1/sample_15.mid (deflated 45%)\n",
      "  adding: RF1/sample_5.mid (deflated 44%)\n",
      "  adding: RF1/sample_8.mid (deflated 46%)\n",
      "  adding: RF1/sample_14.mid (deflated 46%)\n",
      "  adding: RF1/sample_11.mid (deflated 44%)\n",
      "  adding: RF1/sample_10.mid (deflated 44%)\n",
      "  adding: RF1/sample_3.mid (deflated 58%)\n",
      "  adding: RF1/sample_0.mid (deflated 42%)\n",
      "  adding: RF1/sample_6.mid (deflated 48%)\n",
      "  adding: RF1/sample_12.mid (deflated 48%)\n",
      "  adding: RF1/sample_1.mid (deflated 45%)\n",
      "  adding: RF1/sample_2.mid (deflated 47%)\n",
      "  adding: RF1/sample_13.mid (deflated 44%)\n",
      "  adding: RF1/sample_4.mid (deflated 55%)\n",
      "  adding: RF1/sample_9.mid (deflated 49%)\n",
      "  adding: RF1/sample_7.mid (deflated 38%)\n"
     ]
    }
   ],
   "source": [
    "# !rm -rf folder*\n",
    "\n",
    "!zip -r real_midis.zip $(find . -type d -name 'RF*')\n",
    "# !zip -r saved_midis.zip $(find . -type d -name '*_files')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-24T23:24:38.823493Z",
     "iopub.status.idle": "2023-08-24T23:24:38.824376Z",
     "shell.execute_reply": "2023-08-24T23:24:38.824107Z",
     "shell.execute_reply.started": "2023-08-24T23:24:38.824082Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_samples(model, loc):\n",
    "    print(\"-----------------\")\n",
    "    print(loc)\n",
    "    print(\"-----------------\")\n",
    "    generated = generate_new_samples(\n",
    "        best_model,\n",
    "        n_samples=16,\n",
    "        device=device,\n",
    "        h=height\n",
    "    )\n",
    "    \n",
    "    visualise_op(generated)    \n",
    "    \n",
    "#     id = random.randint(0,generated.shape[0]-1)\n",
    "    # id = 3\n",
    "    \n",
    "    for i,audio in enumerate(generated):\n",
    "#         audio = generated[id]\n",
    "        try:\n",
    "            audio = audio.to('cpu').numpy().reshape(192,128)\n",
    "            audio = create_midi(audio, channels=1)\n",
    "            # visualise_op(audio)\n",
    "\n",
    "            audio = midi_post_process(audio)\n",
    "\n",
    "            visualise_op(audio)\n",
    "\n",
    "            df1 = pd.DataFrame(audio.reshape(192,128))\n",
    "#             display(df1)\n",
    "\n",
    "            save_loc = loc + \"/sample_\" + str(i) + \".mid\"\n",
    "\n",
    "            write_midi({'Track1': audio.reshape(192,128).astype(int)}, 4, save_loc, 122)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T14:23:46.813150Z",
     "iopub.status.busy": "2023-08-22T14:23:46.812381Z",
     "iopub.status.idle": "2023-08-22T14:23:58.719460Z",
     "shell.execute_reply": "2023-08-22T14:23:58.718397Z",
     "shell.execute_reply.started": "2023-08-22T14:23:46.813117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘RF4’: File exists\n",
      "397\n",
      "397\n",
      "0\n",
      "397\n",
      "397\n",
      "0\n",
      "397\n",
      "397\n",
      "0\n",
      "397\n",
      "397\n",
      "0\n",
      "397\n",
      "397\n",
      "0\n",
      "397\n",
      "397\n",
      "0\n",
      "397\n",
      "397\n",
      "0\n",
      "397\n",
      "397\n",
      "0\n",
      "397\n",
      "397\n",
      "0\n",
      "397\n",
      "397\n",
      "0\n",
      "397\n",
      "397\n",
      "0\n",
      "397\n",
      "397\n",
      "0\n",
      "397\n",
      "397\n",
      "0\n",
      "397\n",
      "397\n",
      "0\n",
      "397\n",
      "397\n",
      "0\n",
      "397\n",
      "397\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# batc\n",
    "import random\n",
    "\n",
    "for i in range(1):\n",
    "    quadrant = i+4\n",
    "    folder_name = 'RF' + str(quadrant)\n",
    "    !mkdir {folder_name}\n",
    "    for i in range(16):\n",
    "        loader = getDataloader(quadrant)\n",
    "\n",
    "        r_id = random.randint(0,batch_size-1)\n",
    "\n",
    "        audio = None\n",
    "        \n",
    "        for d in loader:\n",
    "            if random.randint(0,batch_size-1) == 9:\n",
    "                break\n",
    "        #     print()\n",
    "            try:\n",
    "                audio = d[0][r_id]\n",
    "            except:\n",
    "                pass\n",
    "        audio = create_midi(audio)\n",
    "        audio = audio.reshape(1,1,192,128)\n",
    "#         visualise_op(audio)\n",
    "\n",
    "        # print(audio)\n",
    "        df = pd.DataFrame(audio.reshape(192,128))\n",
    "#         display(df)\n",
    "        write_midi({'Track1': audio.reshape(192,128)}, 4, folder_name + \"/sample_\" + str(i) + \".mid\", 122)\n",
    "    # !timidity real_in.mid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
